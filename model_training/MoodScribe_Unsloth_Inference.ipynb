{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-_89_GcuYbg8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth \"xformers==0.0.28.post2\"\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELfq_AaFYec6",
        "outputId": "eb184245-28dd-4e2e-cdcd-1e3e6008a880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BvfI9tGpZk5M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "alpaca_prompt = \"\"\"Below is an instruction that provides 10 of the most recent messages from a conversation with time stamps that provides context. Write a response that appropriately completes the request, including the current conversation mood, mood of the response, the next response to send and an engagement score.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEQCfLB2ZG_y",
        "outputId": "9f9de786-4adf-4a9c-b404-11bc0c77c326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "<|begin_of_text|>Below is an instruction that provides 10 of the most recent messages from a conversation with time stamps that provides context. Write a response that appropriately completes the request, including the current conversation mood, mood of the response, the next response to send and an engagement score.\n",
            "\n",
            "### Instruction:\n",
            "Conversation: [2024-11-02T12:00:00Z] [user1]: 'Hey, I got some great news today!' [2024-11-02T12:01:00Z] [user2]: 'Oh, that's awesome! What happened?' [2024-11-02T12:02:00Z] [user1]: 'I finally got that job I interviewed for last week.' [2024-11-02T12:03:00Z] [user2]: 'Congratulations! You totally deserve it.' [2024-11-02T12:04:00Z] [user1]: 'Thank you! I was so nervous.' [2024-11-02T12:05:00Z] [user2]: 'I can imagine. It's such a relief, isn't it?' [2024-11-02T12:06:00Z] [user1]: 'Absolutely. Now I just have to figure out the commute.' [2024-11-02T12:07:00Z] [user2]: 'That's a good problem to have though, right?' [2024-11-02T12:08:00Z] [user1]: 'For sure. I'm just so relieved.' [2024-11-02T12:09:00Z] [user2]: 'I'm really happy for you. Let's celebrate soon!'\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Congrats again! You earned it. Mood: happy. Suggested mood: supportive. Engagement: 10/10.<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"rohansiva/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Conversation: [2024-11-02T12:00:00Z] [user1]: 'Hey, I got some great news today!' [2024-11-02T12:01:00Z] [user2]: 'Oh, that's awesome! What happened?' [2024-11-02T12:02:00Z] [user1]: 'I finally got that job I interviewed for last week.' [2024-11-02T12:03:00Z] [user2]: 'Congratulations! You totally deserve it.' [2024-11-02T12:04:00Z] [user1]: 'Thank you! I was so nervous.' [2024-11-02T12:05:00Z] [user2]: 'I can imagine. It's such a relief, isn't it?' [2024-11-02T12:06:00Z] [user1]: 'Absolutely. Now I just have to figure out the commute.' [2024-11-02T12:07:00Z] [user2]: 'That's a good problem to have though, right?' [2024-11-02T12:08:00Z] [user1]: 'For sure. I'm just so relieved.' [2024-11-02T12:09:00Z] [user2]: 'I'm really happy for you. Let's celebrate soon!'\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1V5MagOZNgu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
